{"0": {
    "doc": "Aggregates",
    "title": "Aggregates",
    "content": "An Aggregate is a calculated entity state, derived from a single Stream.. When you ask EventualizeDB to fetch the state of an entity, in principal what it does is fetching all the Entity’s Stream’s events and “folding” them one on top of the other to derive the current state. Folding means taking a sequence of events and based on their content perform an aggregative calculation. So when a service wants to create an aggregate, it needs to provide to EventualizeDB the identification of the relevant Stream in terms of Stream Type and Stream ID, and a Folding Logic, which is the mapping between an Event Type and the function that holds the desired calculation of its content. That means that you can read the same Stream using different folding logics, and derive different states from the same data! . Another important advantage of this approach is the it provides strong consistency when fetching an aggregate. Because immediately after storing an event, you’ll be able to read it, as the Stream includes that event! . However in order to use it in large scale production systems there are 2 more important considerations: . | Read Time - What if there are many events in an Stream? Wouldn’t reading all of them in order to derive the state take a long time? For that we have Snaphots. | Strong Consistency with Writes - How can an service or application store an event, only if the Stream hasn’t been updated since tha last time it was read? For that we have Optimistic Concurrency Control. | . ",
    "url": "/learn-more/aggregates.html",
    
    "relUrl": "/learn-more/aggregates.html"
  },"1": {
    "doc": "Aggregates",
    "title": "The structure of an Aggregate",
    "content": "| property | data type | meaning | . | Stream Type | string | Required The type of the Stream | . | Stream Id | string | Required The identification of the Stream | . | State | object | Required The current state the aggregate holds | . | Folding Logic | dictionary&lt;event type,folding function&gt; | Required The folding logic that computed the state | . | Pending Events | collection | Required Events that the aggregate holds and have not yet been stored in the Stream. | . | Offset | long | Required The last known Stream Offset by the Aggregate. Used for OCC | . | Min. Events Between Snapshots | int | Required Used for Snapshots | . ",
    "url": "/learn-more/aggregates.html#the-structure-of-an-aggregate",
    
    "relUrl": "/learn-more/aggregates.html#the-structure-of-an-aggregate"
  },"2": {
    "doc": "Code Contribution",
    "title": "Code Contribution",
    "content": " ",
    "url": "/contribution/code-contribution.html",
    
    "relUrl": "/contribution/code-contribution.html"
  },"3": {
    "doc": "Code Contribution",
    "title": "Quick Start",
    "content": "// TODO: . | Install CLI | Create template | Spin Docker or having a connection string to cloud/on-prem DB | . docker run -e \"ACCEPT_EULA=Y\" -e \"MSSQL_SA_PASSWORD=MasadNetunim12!@\" -p 1433:1433 --name sql --hostname sql -d mcr.microsoft.com/mssql/server:2022-latest . | run tests | Run the sample code (F5) | Check the Telemetry | Manual deploy NuGet to nuget.org (first time) | CI/CD Set the permission &amp; secret (will deploy NuGet on each push) | . ",
    "url": "/contribution/code-contribution.html#quick-start",
    
    "relUrl": "/contribution/code-contribution.html#quick-start"
  },"4": {
    "doc": "Events",
    "title": "Events",
    "content": "Events are facts that happened and are of interest to the software system. In principal, anythings that is detected by the software system is an event. Events can be detected from outside of the system, for example: . | User selected a product | Third-party service sent an API request Events can be detected from within the system, for example: | Free trial period ended | User credit score updated As you can see, all the example are phrased in a past tense. This is because events are things that have already happened. | . ",
    "url": "/learn-more/events.html",
    
    "relUrl": "/learn-more/events.html"
  },"5": {
    "doc": "Events",
    "title": "Event Structure",
    "content": "In Eventualize, All events have the same structure: . | property name | data type | meaning | . | event_type | string | required. A description of the event (e.g. “user earned points”) | . | captured_at | datetime | required. The point in time in which the event was detected | . | captured_by | string | required. The name of the service that detected the event | . | payload | json | required. The data of the event (e.g. username, number of points) | . | stored_at | datetime | Optional. Populated by Eventualize after the event has been stored | . ",
    "url": "/learn-more/events.html#event-structure",
    
    "relUrl": "/learn-more/events.html#event-structure"
  },"6": {
    "doc": "Events",
    "title": "Creating a new event",
    "content": "Prerequisite . This section assumes you have installed Eventualize package in you project. If this is not the case, got to Quick Start and learn how to do that. C# . var capturedAt = DateTime.Now; var capturedBy = \"&lt;capturing service name&gt;\"; var data = new EventData(...); var newEvent = new EventEntity(capturedAt, capturedBy,data); . Now that we understand events, we can learn how they are used to derive the system’s state. Hint: this is what Aggregates are for. Let’s proceed to the Aggregates section! . ",
    "url": "/learn-more/events.html#creating-a-new-event",
    
    "relUrl": "/learn-more/events.html#creating-a-new-event"
  },"7": {
    "doc": "Learn More",
    "title": "Learn More",
    "content": "In this section you can get familiar with the Event Sourcing principal of Eentualize and how you can quickly and easly use it yourself. ",
    "url": "/learn-more/",
    
    "relUrl": "/learn-more/"
  },"8": {
    "doc": "Home",
    "title": "Eventualize",
    "content": ". Eventualize is an opinionated event-sourcing framework that unlocks the untapped potential of transactional data while eliminating many challenges related to management of transactional data and its schema. Eventualize is quick &amp; easy to integrate, and while it is working with new paradigms and patterns under the hood, it abstracts most of it away and does not distrupt development. ",
    "url": "/#eventualize",
    
    "relUrl": "/#eventualize"
  },"9": {
    "doc": "Home",
    "title": "Quick Start",
    "content": "If you want to jump right into it, go to Quick Start . ",
    "url": "/#quick-start",
    
    "relUrl": "/#quick-start"
  },"10": {
    "doc": "Home",
    "title": "Learn More",
    "content": "If you want to learn more, go to Learn More. ",
    "url": "/#learn-more",
    
    "relUrl": "/#learn-more"
  },"11": {
    "doc": "Home",
    "title": "Contribute",
    "content": "You can contribute to this project in many ways (not just coding)! If you are interested to learn more about how you can do this, please visit the Contribution page. ",
    "url": "/#contribute",
    
    "relUrl": "/#contribute"
  },"12": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "/",
    
    "relUrl": "/"
  },"13": {
    "doc": "Contribution",
    "title": "Contribution",
    "content": "Eventualize is a project that is meant to not only help integrate event sourcing on a technical level, but also change the paradaigm in which we all think about software systems. There are plenty of ways you can assist us in our mission. Here are some ways that we have thought about. If you don’t find something here that feets you and would like to contribute, contact us at TODO: add email address. ",
    "url": "/contribution/",
    
    "relUrl": "/contribution/"
  },"14": {
    "doc": "Main Mechanisms",
    "title": "Main Mechanisms",
    "content": "This section describes the main mechanisms that are good to be familiar with in order to better understand how EventualizeDB works. ",
    "url": "/learn-more/main-mechanisms/main-mechanisms.html",
    
    "relUrl": "/learn-more/main-mechanisms/main-mechanisms.html"
  },"15": {
    "doc": "Optimistic Concurrency Control (OCC)",
    "title": "Optimistic Concurrency Control (OCC)",
    "content": "Optimistic Concurrency Control is a mechanism Eventualize uses in order to handle the a situation where multiple local aggregates would like to store events into the same stored aggregate. First, let’s understand why is that an issue that needs special handling. ",
    "url": "/learn-more/main-mechanisms/occ.html",
    
    "relUrl": "/learn-more/main-mechanisms/occ.html"
  },"16": {
    "doc": "Optimistic Concurrency Control (OCC)",
    "title": "The Issue of Multiple Writers",
    "content": "Lets assume we have 2 service: service 1 and service 2. Both services created a local aggregate from the same stored aggregate “User: john”. Service 1 added the event “user earned points”. Service 2 added the event “user’s last earning cancelled”. As you can see, the order of stored events is important. If service 2 stores its event first, it’ll cancel the points earned before service 1’s event. If service 1 stores its event fisrt, than the event from service 2 will cancel these earned points. How can we know which service should store its events first? How can we let the second serivce know that it’s going to store events based on a stale knowledge of the current state? . ",
    "url": "/learn-more/main-mechanisms/occ.html#the-issue-of-multiple-writers",
    
    "relUrl": "/learn-more/main-mechanisms/occ.html#the-issue-of-multiple-writers"
  },"17": {
    "doc": "Optimistic Concurrency Control (OCC)",
    "title": "Last Stored Sequence Id",
    "content": "When a local aggregate is created, it stores the sequence id of the latest event that was read into the Last Stored Sequence Id property. Last Stored Sequence Id comes in handy when we want to store the local aggregate and make sure its state is up-to-date. Before actually performing the store operation, Eventualize checks that Last Stored Sequence Id is equal to the sequence id of the latest stored event. If it’s not, that mean that the local aggregate’s request to store pending events is based on a stale knowledge of the current state. Let’s return to our 2 application services example, and add a Last Stored Sequence Id to each local aggregate: . Both local aggregate has a Last Stored Sequence Id with a value of 4. When both of the services try to store their pending events at the same time, one of them bound to be the first to succeed (race conditions). After a successful first store operation, the newly added event will receive the sequence id of 5. That means that the seond service to try and store its pending events will fail, because its Last Stored Sequence Id will still be 4. The mechanism described here is called Optimistic Concurrency Control. The service that fails to store, will receive an OCC Exception. In that case it can refetch the up-to-date state and retry storing the event. ",
    "url": "/learn-more/main-mechanisms/occ.html#last-stored-sequence-id",
    
    "relUrl": "/learn-more/main-mechanisms/occ.html#last-stored-sequence-id"
  },"18": {
    "doc": "Optimistic Concurrency Control (OCC)",
    "title": "Many Multiple Writers",
    "content": "What happens if instead of 2 services writes to the same Aggregate, we have 1,000? An example for that can be a service that receives IoT telemetries at large scale, and might have many instances of itself in order to injust the incoming traffic. If we perform an OCC check for each write, it’ll let a single instance succeed, but will fail the rest of 999 writers. In the next retry 998 writers would fail. And this is neglecting additional writes that might come in during this time. It’ll take forever to complete all the store operations! . Usually, when we have a lot of different writers to the same aggregate, the events can be safely stored regardless of what the previous events are (unlike our 2 services example where the order of storage mattered a lot). For cases like this, you can let Eventualize know that a certain store operation can disregard OCC. When that happens, the events are stored with a sequence id that is incremented automatically for them based on the last sequence id that is already stored in the aggregate. ",
    "url": "/learn-more/main-mechanisms/occ.html#many-multiple-writers",
    
    "relUrl": "/learn-more/main-mechanisms/occ.html#many-multiple-writers"
  },"19": {
    "doc": "Quick Start",
    "title": "Quick Start",
    "content": " ",
    "url": "/quick-start.html",
    
    "relUrl": "/quick-start.html"
  },"20": {
    "doc": "Snapshots",
    "title": "Snapshots",
    "content": "The purpose of a Snapshot is accelarating getting an Aggregate’s state. It does so by optimizing the excution of the FoldingLogic. In essense, a Snapshot is a calculated state that is based on a specific Folding Logic. Whenever events are starting to pile up within an Aggregate, a Snpashot is created. So the next time this aggregate is fetched, the state already exist with no required calculations. If more events are stored since the moment the Snapshot was created, only they are folded unto the the Snapshot’s state. For example, The aggregate of the user “john” holds 1,000 events: . Scanning and folding all these events might take too much time, which can be unreasonable for some use cases. Let’s see what happens when we introduce a Snapshot for the calculating “total points”: . As you can see, a Snapshot is like another type of aggregate. The difference is, that it stores the state across different points in time for a cpecific folding logic of a particular aggregate. In our example, the snapshot stores the different states of folding logic “Total Points” for the user “john” aggregate. The latest state the Snapshot holds is up to event 990. That means it is missing the last 10 events. Let’s see how Eventualize would fetch the “Total Points” for the user “john” now: . ",
    "url": "/learn-more/main-mechanisms/snapshots.html",
    
    "relUrl": "/learn-more/main-mechanisms/snapshots.html"
  },"21": {
    "doc": "Snapshots",
    "title": "When does a Snapshot get created?",
    "content": "Every aggregate has a specification of maximum events between snapshots. When event are stored and the latest snapshot is more than this specification’s number of events - the state of the aggregate is stored as a new snapshot, that is relevant up to the latest event that is now being stored. ",
    "url": "/learn-more/main-mechanisms/snapshots.html#when-does-a-snapshot-get-created",
    
    "relUrl": "/learn-more/main-mechanisms/snapshots.html#when-does-a-snapshot-get-created"
  },"22": {
    "doc": "Streams",
    "title": "Streams",
    "content": "Events that are captured, stored in Streams. A Stream holds in once place events that belong to the same entity. A Stream uses an underlying transactional database of your choice (e.g. SQL Server) to store its events. In order to easily locate an Stream, it has a Stream Type and an Stream Id. The combination of both need to be unique across the system. For example, you can define an Stream of type user with an id of john. Now you can store all events that belong to the user “John” in that Stream. Each stored event gets an Offset which is just a incrementing (long) integer that represents the position of the event within the Stream. ",
    "url": "/learn-more/streams.html",
    
    "relUrl": "/learn-more/streams.html"
  },"23": {
    "doc": "Why Should I Use It?",
    "title": "Why Should I Use It?",
    "content": "In most production applications, transactional data is captured and used to update the state of the system. Here is such an example for a application that updates the amount of points a user has: . The application updates the state of the relevant user record by increasing the points of the user with the additional points: . It’s great for keeping track of the overall points each user has. However, each time we update users points, we overwrite the previous amount of points they had. What if down the line we’d like to see the average number of daily points that earned by the user? Oh… shoot. Too bad we didn’t store the previous amounts in the first place. But that is just it! You can’t anticipate all the things you’d like to do with your data. And along the way you are bound to encounter new data driven questions you haven’t thought of when you were just starting out. Eventualize solves this by storing the actual facts that the system captured over time. We call those Events. So, following the previous example, working with Eventualize in pricnciple looks something like this: . We have captured the fact itself (user 5 earned 5 points): . This is great, beacuse now we can calculate many different things based on these events, without changing the way you store the data. All we need to do is scan of the relevant events that were captured and are related to the user, and aggregate them into the state we are interested in. We call this kind of aggregation a folding logic, and you can have multiple floding logics for the same events in order to derive diiferent states for the system. So, continuing our example, getting the information we want would something like this: . Total amount of points: . Average points per earning: . It’s that easy. Each round node that use saw above is a folding logic, and each box after that node is the resulting state. At this points you probably have some questions. Like: ‘What happens when there are many events that need to be scanned? Wouldn’t that take too long and hurt performance?’ ‘What if the events are stored in the wrong order?’ and many more. There are several mechanisms that needs to be put in place in order for event sourcing to work not only in principal but also in real life. This is why we have built Eventualize - so you wouldn’t have to! The next sections should give you a pretty good understanding on how it all works. So let’s go to the next section! . ",
    "url": "/learn-more/why-should-i-use-it.html",
    
    "relUrl": "/learn-more/why-should-i-use-it.html"
  },"24": {
    "doc": "Working With Stream & Aggregates",
    "title": "Local Aggregate",
    "content": "Up until now, we’ve discussed the aggregate as the place where events of the same entity are stored in. This is the stored aggregate. However for code to effectively execute, it needs to have a local representation of the aggregate, a local aggregate, which can differ, throughout the code’s execution, from the stored aggregate. Let’s see how it looks like and how it is used. ",
    "url": "/learn-more/working-with-stream-and-aggregates.html#local-aggregate",
    
    "relUrl": "/learn-more/working-with-stream-and-aggregates.html#local-aggregate"
  },"25": {
    "doc": "Working With Stream & Aggregates",
    "title": "Creation of local aggregate from a stored aggregate",
    "content": "In order to create a local aggregate, the application code provides to the Eventualize SDK: . | The aggregate type | The aggregate ID | Folding Logic | . Then Eventualize locates the relevant stored aggregate, and the latest snapshot of the provided folding logic (if it exists). Eventualize folds the events since the latest snapshot (if it exists) and retruns a local aggregate with the following values: . | Aggregate Type - As provided by the application | Aggregate Id - As provided by the application | State - The dervied state from folding the events in the stored aggregate | Folding Logic - As provided by the application | Pending Events - An empty collection | . Here is an illustration of the process: . The simpler case is when there is no existing stored aggregate. I that case, the local storage is instantly create with an empty state. ",
    "url": "/learn-more/working-with-stream-and-aggregates.html#creation-of-local-aggregate-from-a-stored-aggregate",
    
    "relUrl": "/learn-more/working-with-stream-and-aggregates.html#creation-of-local-aggregate-from-a-stored-aggregate"
  },"26": {
    "doc": "Working With Stream & Aggregates",
    "title": "Capturing events in a local aggregate",
    "content": "Throughout the application’s execution, it’ll capture or create one or more events. Those events will be appended in the local aggregate to the collection of pending events. This is a very fast operation that can support a high frequency of appends. Here is an illustration for that: . As you can see, the event was added to the pending events collection, and the state was updated by folding the event on top of the previous state. ",
    "url": "/learn-more/working-with-stream-and-aggregates.html#capturing-events-in-a-local-aggregate",
    
    "relUrl": "/learn-more/working-with-stream-and-aggregates.html#capturing-events-in-a-local-aggregate"
  },"27": {
    "doc": "Working With Stream & Aggregates",
    "title": "Storing a local aggregate",
    "content": "The application code captured some events and stored them as pending events in the local aggregate. The state of the local aggregate also have been updated. Now we’d like to store our local aggregate. It is really simply actually: . | The pending events are added to the stored aggregate and removed from the local aggregate (as they are no longer pending storage). | If a snapshot should be created, the current state of the local aggregate is stored as a snapshot for the relevant folding logic and event. Here is an illustration of that: | . ",
    "url": "/learn-more/working-with-stream-and-aggregates.html#storing-a-local-aggregate",
    
    "relUrl": "/learn-more/working-with-stream-and-aggregates.html#storing-a-local-aggregate"
  },"28": {
    "doc": "Working With Stream & Aggregates",
    "title": "Working With Stream & Aggregates",
    "content": " ",
    "url": "/learn-more/working-with-stream-and-aggregates.html",
    
    "relUrl": "/learn-more/working-with-stream-and-aggregates.html"
  }
}
