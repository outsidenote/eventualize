{"0": {
    "doc": "Aggregates",
    "title": "Aggregates",
    "content": "An Aggregate represents a local copy of an Entity’s state. It is derived from a the entity’s Stream. Usually, the application code will fetch an aggregate to: . | Fetch the current state on an Entity. | Store new events to the Entity’s Stream. | . ",
    "url": "/learn-more/aggregates.html",
    
    "relUrl": "/learn-more/aggregates.html"
  },"1": {
    "doc": "Aggregates",
    "title": "Creating an Aggregate",
    "content": "You create an aggregate in order to fetch the current state of an Entity. When you ask EventualizeDB to create an aggregate, what it does is: . | Fetch all the events that are stored in the Entity’s Stream | “Fold” the fetched events one on top of the other to derive the current state. | . To Fold means taking a sequence of events and based on their content perform an aggregative calculation. The result of this calculation is the State. So when a service wants to create an aggregate, it needs to provide to EventualizeDB: . | The identification of the Entity’s Stream in terms of Stream Type and Stream ID | Folding Logic, which is the mapping between an Event Type and the Folding Function that holds the desired calculation the state based on the event’s content. | . That means that you can read the same Stream using different Folding Logics, and derive different States from the same Stream! . Another important advantage of this approach is the it provides strong consistency when creating an Aggregate. This is because immediately after storing an Event, you’ll be able to read it, as the Stream includes that Event! . ",
    "url": "/learn-more/aggregates.html#creating-an-aggregate",
    
    "relUrl": "/learn-more/aggregates.html#creating-an-aggregate"
  },"2": {
    "doc": "Aggregates",
    "title": "Capturing Events in an Aggregate",
    "content": "Throughout the application’s execution, it’ll capture or create one or more events. Captured events are not stored directly into the Stream, because it can heart performance. Those events will be appended to the ordered collection of Pending Events in the Aggregate. Pending Events are events that were captured locally and not yet stored in the Stream. The Folding Logic will also execute the relevant Folding Function, based on the Event’s type. This will update the Aggregate’s State. Capturing Events is a fast operation that can support a high frequency of appends. Here is an illustration for that: . As you can see, the Event was added to the Pending Events collection, and the State was updated by folding the Event on top of the previous State. ",
    "url": "/learn-more/aggregates.html#capturing-events-in-an-aggregate",
    
    "relUrl": "/learn-more/aggregates.html#capturing-events-in-an-aggregate"
  },"3": {
    "doc": "Aggregates",
    "title": "Storing a local Aggregate",
    "content": "The application code captured some Events and appeneded them to the Aggregate’s Pending Events. The state of Aggregate also have been updated. Now we’d like to store those Pending Events into to the respective Stream. It is really simply actually: . | The Pending Events are added to the Stream and removed from the Aggregate (as they are no longer pending). | If a Snapshot should be created, the current State of the Aggregate is stored as a Snapshot and assigned with the Offset of the latest stored Event. Here is an illustration of that: | . ",
    "url": "/learn-more/aggregates.html#storing-a-local-aggregate",
    
    "relUrl": "/learn-more/aggregates.html#storing-a-local-aggregate"
  },"4": {
    "doc": "Aggregates",
    "title": "Production Workload",
    "content": "In order to use Aggregates in a high performing production system, there are 2 additional considerations: . | Read Time - What if there are many events in an Stream? Wouldn’t reading all of them in order to derive the state take a long time? For that we have Snaphots. | Strong Consistency with Writes - How can an service or application store an event, only if the Stream hasn’t been updated since tha last time it was read? For that we have Optimistic Concurrency Control. | . ",
    "url": "/learn-more/aggregates.html#production-workload",
    
    "relUrl": "/learn-more/aggregates.html#production-workload"
  },"5": {
    "doc": "Aggregates",
    "title": "The structure of an Aggregate",
    "content": "| property | data type | meaning | . | Stream Type | string | Required The type of the Stream | . | Stream Id | string | Required The identification of the Stream | . | State | object | Required The current state the aggregate holds | . | Folding Logic | dictionary&lt;event type,folding function&gt; | Required The folding logic that computed the state | . | Pending Events | collection | Required Events that the aggregate holds and have not yet been stored in the Stream. | . | Offset | long | Required The last known Stream Offset by the Aggregate. Used for OCC | . | Min. Events Between Snapshots | int | Required Used for Snapshots | . ",
    "url": "/learn-more/aggregates.html#the-structure-of-an-aggregate",
    
    "relUrl": "/learn-more/aggregates.html#the-structure-of-an-aggregate"
  },"6": {
    "doc": "Code Contribution",
    "title": "Code Contribution",
    "content": " ",
    "url": "/contribution/code-contribution.html",
    
    "relUrl": "/contribution/code-contribution.html"
  },"7": {
    "doc": "Code Contribution",
    "title": "Quick Start",
    "content": "// TODO: . | Install CLI | Create template | Spin Docker or having a connection string to cloud/on-prem DB | . docker run -e \"ACCEPT_EULA=Y\" -e \"MSSQL_SA_PASSWORD=MasadNetunim12!@\" -p 1433:1433 --name sql --hostname sql -d mcr.microsoft.com/mssql/server:2022-latest . | run tests | Run the sample code (F5) | Check the Telemetry | Manual deploy NuGet to nuget.org (first time) | CI/CD Set the permission &amp; secret (will deploy NuGet on each push) | . ",
    "url": "/contribution/code-contribution.html#quick-start",
    
    "relUrl": "/contribution/code-contribution.html#quick-start"
  },"8": {
    "doc": "Events",
    "title": "Events",
    "content": "Events are facts that happened and are of interest to the software system. In principal, anythings that is detected by the software system is an event. Events can be detected from outside of the system, for example: . | User selected a product | Third-party service sent an API request Events can be detected from within the system, for example: | Free trial period ended | User credit score updated As you can see, all the example are phrased in a past tense. This is because events are things that have already happened. | . ",
    "url": "/learn-more/events.html",
    
    "relUrl": "/learn-more/events.html"
  },"9": {
    "doc": "Events",
    "title": "Event Structure",
    "content": "In Eventualize, All events have the same structure: . | property name | data type | meaning | . | event_type | string | required. A description of the event (e.g. “user earned points”) | . | captured_at | datetime | required. The point in time in which the event was detected | . | captured_by | string | required. The name of the service that detected the event | . | payload | json | required. The data of the event (e.g. username, number of points) | . | stored_at | datetime | Optional. Populated by Eventualize after the event has been stored | . Now that we understand events, we can learn how they are used to derive the system’s state. Hint: this is what Aggregates are for. Let’s proceed to the Aggregates section! . ",
    "url": "/learn-more/events.html#event-structure",
    
    "relUrl": "/learn-more/events.html#event-structure"
  },"10": {
    "doc": "Home",
    "title": "Quick Start",
    "content": "If you want to jump right into it, go to Quick Start . ",
    "url": "/#quick-start",
    
    "relUrl": "/#quick-start"
  },"11": {
    "doc": "Home",
    "title": "Learn More",
    "content": "If you want to learn more, go to Learn More. ",
    "url": "/#learn-more",
    
    "relUrl": "/#learn-more"
  },"12": {
    "doc": "Home",
    "title": "Contribute",
    "content": "You can contribute to this project in many ways (not just coding)! If you are interested to learn more about how you can do this, please visit the Contribution page. ",
    "url": "/#contribute",
    
    "relUrl": "/#contribute"
  },"13": {
    "doc": "Home",
    "title": "Home",
    "content": ". Eventualize is an opinionated event-sourcing framework that unlocks the untapped potential of transactional data while eliminating many challenges related to management of transactional data and its schema. Eventualize is quick &amp; easy to integrate, and while it is working with new paradigms and patterns under the hood, it abstracts most of it away and does not distrupt development. ",
    "url": "/",
    
    "relUrl": "/"
  },"14": {
    "doc": "Learn More",
    "title": "Learn More",
    "content": "In this section you can get familiar with the Event Sourcing principal of Eentualize and how you can quickly and easly use it yourself. ",
    "url": "/learn-more/",
    
    "relUrl": "/learn-more/"
  },"15": {
    "doc": "Contribution",
    "title": "Contribution",
    "content": "Eventualize is a project that is meant to not only help integrate event sourcing on a technical level, but also change the paradaigm in which we all think about software systems. There are plenty of ways you can assist us in our mission. Here are some ways that we have thought about. If you don’t find something here that feets you and would like to contribute, contact us at TODO: add email address. ",
    "url": "/contribution/",
    
    "relUrl": "/contribution/"
  },"16": {
    "doc": "Main Mechanisms",
    "title": "Main Mechanisms",
    "content": "This section describes the main mechanisms that are good to be familiar with in order to better understand how EventualizeDB works. ",
    "url": "/learn-more/main-mechanisms/main-mechanisms.html",
    
    "relUrl": "/learn-more/main-mechanisms/main-mechanisms.html"
  },"17": {
    "doc": "Optimistic Concurrency Control (OCC)",
    "title": "Optimistic Concurrency Control (OCC)",
    "content": "Optimistic Concurrency Control is a mechanism Eventualize uses in order to handle the a situation where multiple local aggregates would like to store events into the same stored aggregate. First, let’s understand why is that an issue that needs special handling. ",
    "url": "/learn-more/main-mechanisms/occ.html",
    
    "relUrl": "/learn-more/main-mechanisms/occ.html"
  },"18": {
    "doc": "Optimistic Concurrency Control (OCC)",
    "title": "The Issue of Multiple Writers",
    "content": "Lets assume we have 2 service: service 1 and service 2. Both services created a local aggregate from the same stored aggregate “User: john”. Service 1 added the event “user earned points”. Service 2 added the event “user’s last earning cancelled”. As you can see, the order of stored events is important. If service 2 stores its event first, it’ll cancel the points earned before service 1’s event. If service 1 stores its event fisrt, than the event from service 2 will cancel these earned points. How can we know which service should store its events first? How can we let the second serivce know that it’s going to store events based on a stale knowledge of the current state? . ",
    "url": "/learn-more/main-mechanisms/occ.html#the-issue-of-multiple-writers",
    
    "relUrl": "/learn-more/main-mechanisms/occ.html#the-issue-of-multiple-writers"
  },"19": {
    "doc": "Optimistic Concurrency Control (OCC)",
    "title": "Last Stored Sequence Id",
    "content": "When a local aggregate is created, it stores the sequence id of the latest event that was read into the Last Stored Sequence Id property. Last Stored Sequence Id comes in handy when we want to store the local aggregate and make sure its state is up-to-date. Before actually performing the store operation, Eventualize checks that Last Stored Sequence Id is equal to the sequence id of the latest stored event. If it’s not, that mean that the local aggregate’s request to store pending events is based on a stale knowledge of the current state. Let’s return to our 2 application services example, and add a Last Stored Sequence Id to each local aggregate: . Both local aggregate has a Last Stored Sequence Id with a value of 4. When both of the services try to store their pending events at the same time, one of them bound to be the first to succeed (race conditions). After a successful first store operation, the newly added event will receive the sequence id of 5. That means that the seond service to try and store its pending events will fail, because its Last Stored Sequence Id will still be 4. The mechanism described here is called Optimistic Concurrency Control. The service that fails to store, will receive an OCC Exception. In that case it can refetch the up-to-date state and retry storing the event. ",
    "url": "/learn-more/main-mechanisms/occ.html#last-stored-sequence-id",
    
    "relUrl": "/learn-more/main-mechanisms/occ.html#last-stored-sequence-id"
  },"20": {
    "doc": "Optimistic Concurrency Control (OCC)",
    "title": "Many Multiple Writers",
    "content": "What happens if instead of 2 services writes to the same Aggregate, we have 1,000? An example for that can be a service that receives IoT telemetries at large scale, and might have many instances of itself in order to injust the incoming traffic. If we perform an OCC check for each write, it’ll let a single instance succeed, but will fail the rest of 999 writers. In the next retry 998 writers would fail. And this is neglecting additional writes that might come in during this time. It’ll take forever to complete all the store operations! . Usually, when we have a lot of different writers to the same aggregate, the events can be safely stored regardless of what the previous events are (unlike our 2 services example where the order of storage mattered a lot). For cases like this, you can let Eventualize know that a certain store operation can disregard OCC. When that happens, the events are stored with a sequence id that is incremented automatically for them based on the last sequence id that is already stored in the aggregate. ",
    "url": "/learn-more/main-mechanisms/occ.html#many-multiple-writers",
    
    "relUrl": "/learn-more/main-mechanisms/occ.html#many-multiple-writers"
  },"21": {
    "doc": "Quick Start",
    "title": "Quick Start",
    "content": " ",
    "url": "/quick-start.html",
    
    "relUrl": "/quick-start.html"
  },"22": {
    "doc": "Snapshots",
    "title": "Snapshots",
    "content": "The purpose of a Snapshot is accelarating getting an Aggregate’s state. It does so by optimizing the excution of the FoldingLogic. In essense, a Snapshot is a calculated state that is based on a specific Folding Logic. Whenever events are starting to pile up within an Aggregate, a Snpashot is created. So the next time this aggregate is fetched, the state already exist with no required calculations. If more events are stored since the moment the Snapshot was created, only they are folded unto the the Snapshot’s state. For example, The aggregate of the user “john” holds 1,000 events: . Scanning and folding all these events might take too much time, which can be unreasonable for some use cases. Let’s see what happens when we introduce a Snapshot for the calculating “total points”: . As you can see, a Snapshot is like another type of aggregate. The difference is, that it stores the state across different points in time for a cpecific folding logic of a particular aggregate. In our example, the snapshot stores the different states of folding logic “Total Points” for the user “john” aggregate. The latest state the Snapshot holds is up to event 990. That means it is missing the last 10 events. Let’s see how Eventualize would fetch the “Total Points” for the user “john” now: . ",
    "url": "/learn-more/main-mechanisms/snapshots.html",
    
    "relUrl": "/learn-more/main-mechanisms/snapshots.html"
  },"23": {
    "doc": "Snapshots",
    "title": "When does a Snapshot get created?",
    "content": "Every aggregate has a specification of maximum events between snapshots. When event are stored and the latest snapshot is more than this specification’s number of events - the state of the aggregate is stored as a new snapshot, that is relevant up to the latest event that is now being stored. ",
    "url": "/learn-more/main-mechanisms/snapshots.html#when-does-a-snapshot-get-created",
    
    "relUrl": "/learn-more/main-mechanisms/snapshots.html#when-does-a-snapshot-get-created"
  },"24": {
    "doc": "Streams",
    "title": "Streams",
    "content": "Events that are captured, stored in Streams. A Stream holds in once place events that belong to the same entity. A Stream uses an underlying transactional database of your choice (e.g. SQL Server) to store its events. In order to easily locate a Stream, it has a Stream Type and a Stream Id. The combination of both needs to be unique across the system. For example, you can define a Stream of type user with an id of John. Now you can store all events that belong to the user “John” in that Stream. Each stored event gets an Offset, which is just an incrementing (long) integer that represents the position of the event within the Stream. If you have a lot of entities (e.g. users), you’ll have a lot of Streams. That’s ok, as Steams are lightweight and performant. Other than storing events, you would probably also like to aggregate them into different states. This is what Aggregates are for. ",
    "url": "/learn-more/streams.html",
    
    "relUrl": "/learn-more/streams.html"
  },"25": {
    "doc": "Why Should I Use It?",
    "title": "Why Should I Use It?",
    "content": "In most production applications, transactional data is captured and used to update the state of the system. Here is such an example for a application that updates the amount of points a user has: . The application updates the state of the relevant user record by increasing the points of the user with the additional points: . It’s great for keeping track of the overall points each user has. However, each time we update users points, we overwrite the previous amount of points they had. What if down the line we’d like to see the average number of daily points that earned by the user? Oh… shoot. Too bad we didn’t store the previous amounts in the first place. But that is just it! You can’t anticipate all the things you’d like to do with your data. And along the way you are bound to encounter new data driven questions you haven’t thought of when you were just starting out. Eventualize solves this by storing the actual facts that the system captured over time. We call those Events. So, following the previous example, working with Eventualize in principle looks something like this: . We have captured the fact itself (user 5 earned 5 points): . This is great, because now we can calculate many different things based on these events, without changing the way you store the data. All we need to do is scan of the relevant events that were captured and are related to the user, and aggregate them into the state we are interested in. We call this kind of aggregation a folding logic, and you can have multiple folding logics for the same events in order to derive different states for the system. So, continuing our example, getting the information we want would something like this: . Total amount of points: . Average points per earning: . It’s that easy. Each pink circle that you saw above is a folding logic, and each box after that node is the resulting state. At this point, you probably have some questions. Like: ‘What happens when there are many events that need to be scanned? Wouldn’t that take too long and hurt performance?’ ‘What if the events are stored in the wrong order?’ and many more. There are several mechanisms that need to be put in place in order for event sourcing to work not only in principal but also in real life. This is why we have built Eventualize - so you wouldn’t have to! The next sections should give you a pretty good understanding on how it all works. So let’s go to the next section! . ",
    "url": "/learn-more/why-should-i-use-it.html",
    
    "relUrl": "/learn-more/why-should-i-use-it.html"
  }
}
